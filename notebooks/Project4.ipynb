{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>season</th>\n",
       "      <th>neutral</th>\n",
       "      <th>playoff</th>\n",
       "      <th>team1</th>\n",
       "      <th>team2</th>\n",
       "      <th>elo1</th>\n",
       "      <th>elo2</th>\n",
       "      <th>elo_prob1</th>\n",
       "      <th>score1</th>\n",
       "      <th>score2</th>\n",
       "      <th>result1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1920-09-26</td>\n",
       "      <td>1920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RII</td>\n",
       "      <td>STP</td>\n",
       "      <td>1503.947000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>0.824651</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1920-10-03</td>\n",
       "      <td>1920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AKR</td>\n",
       "      <td>WHE</td>\n",
       "      <td>1503.420000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>0.824212</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1920-10-03</td>\n",
       "      <td>1920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RCH</td>\n",
       "      <td>ABU</td>\n",
       "      <td>1503.420000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>0.824212</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1920-10-03</td>\n",
       "      <td>1920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DAY</td>\n",
       "      <td>COL</td>\n",
       "      <td>1493.002000</td>\n",
       "      <td>1504.908000</td>\n",
       "      <td>0.575819</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1920-10-03</td>\n",
       "      <td>1920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RII</td>\n",
       "      <td>MUN</td>\n",
       "      <td>1516.108000</td>\n",
       "      <td>1478.004000</td>\n",
       "      <td>0.644171</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16269</th>\n",
       "      <td>2019-01-13</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NE</td>\n",
       "      <td>LAC</td>\n",
       "      <td>1640.171960</td>\n",
       "      <td>1647.624483</td>\n",
       "      <td>0.582068</td>\n",
       "      <td>41</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16270</th>\n",
       "      <td>2019-01-13</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "      <td>PHI</td>\n",
       "      <td>1669.105861</td>\n",
       "      <td>1633.114673</td>\n",
       "      <td>0.641378</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16271</th>\n",
       "      <td>2019-01-20</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "      <td>LAR</td>\n",
       "      <td>1682.450194</td>\n",
       "      <td>1648.424105</td>\n",
       "      <td>0.638772</td>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16272</th>\n",
       "      <td>2019-01-20</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>KC</td>\n",
       "      <td>NE</td>\n",
       "      <td>1675.286412</td>\n",
       "      <td>1661.668566</td>\n",
       "      <td>0.611248</td>\n",
       "      <td>31</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16273</th>\n",
       "      <td>2019-02-03</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>LAR</td>\n",
       "      <td>NE</td>\n",
       "      <td>1666.969395</td>\n",
       "      <td>1686.338837</td>\n",
       "      <td>0.472154</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16274 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  season  neutral  playoff team1 team2         elo1  \\\n",
       "0      1920-09-26    1920        0        0   RII   STP  1503.947000   \n",
       "1      1920-10-03    1920        0        0   AKR   WHE  1503.420000   \n",
       "2      1920-10-03    1920        0        0   RCH   ABU  1503.420000   \n",
       "3      1920-10-03    1920        0        0   DAY   COL  1493.002000   \n",
       "4      1920-10-03    1920        0        0   RII   MUN  1516.108000   \n",
       "...           ...     ...      ...      ...   ...   ...          ...   \n",
       "16269  2019-01-13    2018        0        1    NE   LAC  1640.171960   \n",
       "16270  2019-01-13    2018        0        1    NO   PHI  1669.105861   \n",
       "16271  2019-01-20    2018        0        1    NO   LAR  1682.450194   \n",
       "16272  2019-01-20    2018        0        1    KC    NE  1675.286412   \n",
       "16273  2019-02-03    2018        1        1   LAR    NE  1666.969395   \n",
       "\n",
       "              elo2  elo_prob1  score1  score2  result1  \n",
       "0      1300.000000   0.824651      48       0      1.0  \n",
       "1      1300.000000   0.824212      43       0      1.0  \n",
       "2      1300.000000   0.824212      10       0      1.0  \n",
       "3      1504.908000   0.575819      14       0      1.0  \n",
       "4      1478.004000   0.644171      45       0      1.0  \n",
       "...            ...        ...     ...     ...      ...  \n",
       "16269  1647.624483   0.582068      41      28      1.0  \n",
       "16270  1633.114673   0.641378      20      14      1.0  \n",
       "16271  1648.424105   0.638772      23      26      0.0  \n",
       "16272  1661.668566   0.611248      31      37      0.0  \n",
       "16273  1686.338837   0.472154       3      13      0.0  \n",
       "\n",
       "[16274 rows x 12 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl_games = pd.read_csv('../data/nfl_games.csv')\n",
    "scores = nfl_games[['score1', 'score2']]\n",
    "nfl_games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring The Data\n",
    "\n",
    "We do a little bit of data exploration by taking the averages of regular season and playoff scores and notice a small difference in the home teams score. Then just for fun we look at the total number of teams that have existed in the NFL. Interestingly enough many teams played only 1 or 2 official games. I considered cutting these teams but figured that getting comparative elo scores would make them useful data points. This turned out to not work out too well because in later tests we find elo to not correlate to score very well on its own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score1    21.544058\n",
      "score2    18.578161\n",
      "dtype: float64\n",
      "score1    21.441877\n",
      "score2    18.583079\n",
      "dtype: float64\n",
      "score1    24.379859\n",
      "score2    18.441696\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(nfl_games[['score1', 'score2']].mean())\n",
    "print(nfl_games[nfl_games['playoff'] == 0][['score1', 'score2']].mean())\n",
    "print(nfl_games[nfl_games['playoff'] == 1][['score1', 'score2']].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home Teams: 101\n",
      "Away Teams: 108\n",
      "All  Teams: 123\n"
     ]
    }
   ],
   "source": [
    "print('Home Teams:', len(nfl_games['team1'].unique()))\n",
    "print('Away Teams:', len(nfl_games['team2'].unique()))\n",
    "print('All  Teams:', len(nfl_games['team1'].append(nfl_games['team2']).unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making new features\n",
    "The features added to the original data set are simple. First splitting the dates up just to deal with the numbers individually. I also calculated the difference in elo between teams. The other added features were the season average of a team in its home and away games and the previous season's average of the team's home and away games. I added the season and previous season averages after doing some regressions and found that the previous season's average had almost no bearing on the predictive strength of the model, but current season averages did. Another interesting feature to add would be the cumulative season average for each team as each game was played, but I don't have the time and pandas-fu to put that together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Month Day Year\n",
    "dates_split = nfl_games['date'].apply(lambda x: x.split('-'))\n",
    "year  = dates_split.apply(lambda x: x[0])\n",
    "month = dates_split.apply(lambda x: x[1])\n",
    "day   = dates_split.apply(lambda x: x[2])\n",
    "dates_df = pd.DataFrame({'year':year, 'month':month, 'day':day})\n",
    "nfl_games = dates_df.join(nfl_games.drop('date', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elo difference between home and away team\n",
    "elo_diff = nfl_games['elo1'] - nfl_games['elo2']\n",
    "nfl_games['elo_diff'] = elo_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = nfl_games['season'].unique()\n",
    "teams = nfl_games['team1'].append(nfl_games['team2']).unique()\n",
    "\n",
    "rows1 = []\n",
    "rows2 = []\n",
    "for s in seasons:\n",
    "    row1 = []\n",
    "    row2 = []\n",
    "    for t in teams:\n",
    "        row1.append(nfl_games[(nfl_games['season'] == s) & (nfl_games['team1'] == t)]['score1'].mean())\n",
    "        row2.append(nfl_games[(nfl_games['season'] == s) & (nfl_games['team2'] == t)]['score2'].mean())\n",
    "    rows1.append(row1)\n",
    "    rows2.append(row2)\n",
    "    \n",
    "team1_avg = pd.DataFrame(rows1, index=seasons, columns=teams)\n",
    "team1_avg = team1_avg.fillna(0)\n",
    "\n",
    "team2_avg = pd.DataFrame(rows2, index=seasons, columns=teams)\n",
    "team2_avg = team2_avg.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "team1_ps_avg = []\n",
    "team2_ps_avg = []\n",
    "\n",
    "for s,t in nfl_games[nfl_games['season'] != 1920][['season', 'team1']].values:        \n",
    "    team1_ps_avg.append(team1_avg[t][s-1])\n",
    "\n",
    "for s,t in nfl_games[nfl_games['season'] != 1920][['season', 'team2']].values:        \n",
    "    team2_ps_avg.append(team2_avg[t][s-1])\n",
    "    \n",
    "first_season = [0] * (len(nfl_games) - len(team1_ps_avg))\n",
    "\n",
    "team1_ps_avg = first_season + team1_ps_avg\n",
    "team2_ps_avg = first_season + team2_ps_avg\n",
    "\n",
    "team1_ps_avg = pd.Series(team1_ps_avg, name='team1_ps_avg')\n",
    "team2_ps_avg = pd.Series(team2_ps_avg, name='team2_ps_avg')\n",
    "\n",
    "nfl_games['team1_ps_avg'] = team1_ps_avg\n",
    "nfl_games['team2_ps_avg'] = team2_ps_avg\n",
    "\n",
    "team1_c_avg = []\n",
    "team2_c_avg = []\n",
    "\n",
    "for s,t in nfl_games[['season', 'team1']].values:        \n",
    "    team1_c_avg.append(team1_avg[t][s])\n",
    "\n",
    "for s,t in nfl_games[['season', 'team2']].values:        \n",
    "    team2_c_avg.append(team2_avg[t][s])\n",
    "\n",
    "team1_c_avg = pd.Series(team1_c_avg, name='team1_avg')\n",
    "team2_c_avg = pd.Series(team2_c_avg, name='team2_avg')\n",
    "\n",
    "nfl_games['team1_avg'] = team1_c_avg\n",
    "nfl_games['team2_avg'] = team2_c_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressions\n",
    "\n",
    "To start the regressions I wanted to see whether or not one hot encoding the team name variable made any difference. I found that one hot encodings had a marginally better mean squared error, but took longer to compute, so for all other regressions I used the label encodings. As for the model, I started with the base Random Forest Regressor from scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encode Teams\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(nfl_games['team1'].append(nfl_games['team2']))\n",
    "\n",
    "team1_le = pd.DataFrame(le.transform(nfl_games['team1']), columns=['team1_cat'])\n",
    "team2_le = pd.DataFrame(le.transform(nfl_games['team2']), columns=['team2_cat'])\n",
    "\n",
    "nfl_games_le = nfl_games.join(team1_le).join(team2_le).drop('team1', axis=1).drop('team2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding of teams\n",
    "nfl_one_hot_1 = pd.get_dummies(nfl_games, columns=['team1'], prefix=['Team_1_is'] )\n",
    "nfl_games_oe = pd.get_dummies(nfl_one_hot_1, columns=['team2'], prefix=['Team_2_is'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_data_random_forest(data):\n",
    "    features = pd.DataFrame(data.drop(['score1', 'score2'], axis=1))\n",
    "    labels = data[['score1', 'score2']]\n",
    "    \n",
    "    rf = RandomForestRegressor()\n",
    "    rf.fit(features.values, labels.values)\n",
    "    return rf\n",
    "\n",
    "def predict_data(model, data):\n",
    "    return model.predict(pd.DataFrame(data.drop(['score1', 'score2'], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.99543534111866"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try with label encodings\n",
    "training_le = nfl_games_le.sample(frac=0.90, random_state=1)\n",
    "testing_le = nfl_games_le.drop(training_le.index)\n",
    "\n",
    "rf_le = fit_data_random_forest(training_le)\n",
    "pred_le = predict_data(rf_le, testing_le)\n",
    "mean_squared_error(testing_le[['score1','score2']], pred_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.562776551936075"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try with one hot encodings\n",
    "training_oe = nfl_games_oe.sample(frac=0.90, random_state=1)\n",
    "testing_oe = nfl_games_oe.drop(training_oe.index)\n",
    "\n",
    "rf_oe = fit_data_random_forest(training_oe)\n",
    "pred_oe = predict_data(rf_oe, testing_oe)\n",
    "mean_squared_error(testing_oe[['score1','score2']], pred_oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_games = nfl_games_le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different subsets of data\n",
    "\n",
    "To see which variables made the biggest impact on deciding the scores, I took a variety of features and ran them through regressions on their own. I compared the mean squared error to the mean squared error of simply taking the average over all seasons. The surprising value that jumps out is that elo and elo related features were the worst when taken on their own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = nfl_games.sample(frac=0.90, random_state=1)\n",
    "testing = nfl_games.drop(training.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121.12152427236573\n"
     ]
    }
   ],
   "source": [
    "# pure averages\n",
    "avg = training[['score1','score2']].mean()\n",
    "avg_df = pd.DataFrame({'score1':avg[0], 'score2':avg[1]}, index=[0])\n",
    "avgs = pd.concat([avg_df]*(len(testing)), ignore_index=True)\n",
    "print(mean_squared_error(testing[['score1','score2']], avgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130.00449103750458\n"
     ]
    }
   ],
   "source": [
    "# elos only\n",
    "feats = ['elo1', 'elo2', 'score1', 'score2']\n",
    "rf_elo = fit_data_random_forest(training[feats])\n",
    "pred_elo = predict_data(rf_elo, testing[feats])\n",
    "print(mean_squared_error(testing[['score1','score2']], pred_elo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109.63884290741646\n"
     ]
    }
   ],
   "source": [
    "# season only\n",
    "feats = ['season', 'score1', 'score2']\n",
    "rf_elo = fit_data_random_forest(training[feats])\n",
    "pred_elo = predict_data(rf_elo, testing[feats])\n",
    "print(mean_squared_error(testing[['score1','score2']], pred_elo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119.89509037526537\n"
     ]
    }
   ],
   "source": [
    "# teams only\n",
    "feats = ['team1_cat', 'team2_cat', 'score1', 'score2']\n",
    "rf_elo = fit_data_random_forest(training[feats])\n",
    "pred_elo = predict_data(rf_elo, testing[feats])\n",
    "print(mean_squared_error(testing[['score1','score2']], pred_elo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125.328981832238\n"
     ]
    }
   ],
   "source": [
    "# date only\n",
    "feats = ['month', 'day', 'year', 'score1', 'score2']\n",
    "rf_elo = fit_data_random_forest(training[feats])\n",
    "pred_elo = predict_data(rf_elo, testing[feats])\n",
    "print(mean_squared_error(testing[['score1','score2']], pred_elo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125.6995814938548\n"
     ]
    }
   ],
   "source": [
    "# date but season only\n",
    "feats = ['month', 'day', 'season', 'score1', 'score2']\n",
    "rf_elo = fit_data_random_forest(training[feats])\n",
    "pred_elo = predict_data(rf_elo, testing[feats])\n",
    "print(mean_squared_error(testing[['score1','score2']], pred_elo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.90832509910177\n"
     ]
    }
   ],
   "source": [
    "# playoffs\n",
    "feats = ['playoff', 'score1', 'score2']\n",
    "rf_elos = fit_data_random_forest(training[feats])\n",
    "pred_elos = predict_data(rf_elos, testing[feats])\n",
    "print(mean_squared_error(testing[['score1','score2']], pred_elos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121.02809200328338\n"
     ]
    }
   ],
   "source": [
    "# neutral\n",
    "feats = ['neutral', 'score1', 'score2']\n",
    "rf_elos = fit_data_random_forest(training[feats])\n",
    "pred_elos = predict_data(rf_elos, testing[feats])\n",
    "print(mean_squared_error(testing[['score1','score2']], pred_elos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162.978346242261\n"
     ]
    }
   ],
   "source": [
    "feats = ['elo_diff', 'score1', 'score2']\n",
    "rf_elo_diff = fit_data_random_forest(training[feats])\n",
    "pred_elo_diff = predict_data(rf_elo_diff, testing[feats])\n",
    "print(mean_squared_error(testing[['score1','score2']], pred_elo_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131.54608452790413\n"
     ]
    }
   ],
   "source": [
    "feats = ['elo1', 'elo2', 'elo_diff', 'elo_prob1', 'score1', 'score2']\n",
    "rf_elos = fit_data_random_forest(training[feats])\n",
    "pred_elos = predict_data(rf_elos, testing[feats])\n",
    "print(mean_squared_error(testing[['score1','score2']], pred_elos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Gradient Boosting\n",
    "\n",
    "To test another model, I tried the gradient boosting regressor from scikit-learn. This model performed marginally better than the random forest, but takes longer to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_data_gb(data):\n",
    "    features = pd.DataFrame(data.drop(['score1', 'score2'], axis=1))\n",
    "    labels1 = data[['score1']]\n",
    "    labels2 = data[['score2']]\n",
    "    \n",
    "    gb1 = GradientBoostingRegressor()\n",
    "    gb2 = GradientBoostingRegressor()\n",
    "    gb1.fit(features.values, labels1.values)\n",
    "    gb2.fit(features.values, labels2.values)\n",
    "    return (gb1, gb2)\n",
    "\n",
    "def predict_data_gb(model, data):\n",
    "    df = pd.DataFrame(data.drop(['score1', 'score2'], axis=1))\n",
    "    return (model[0].predict(df.values), model[1].predict(df.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/usr/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "training = nfl_games_le.sample(frac=0.90, random_state=1)\n",
    "testing = nfl_games_le.drop(training.index)\n",
    "\n",
    "gb = fit_data_gb(training)\n",
    "pred_gb = predict_data_gb(gb, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.14200950971905\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(testing[['score1','score2']], list(zip(pred_gb[0], pred_gb[1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts\n",
    "\n",
    "Overall, I was surprised by how little ELO helped determine scores. Looking at what ELO actually is, it makes more sense. Two teams with the same ELO could have wildly different ways of winning. It would be interesting to add data regarding rosters and coaching staff to the models. I think that this type of data would go a long way towards determining scores. Also of note is that my models are not very good for future predictions because of the season average score for each team. This data can't exist until a full season has been played. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
